{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b17e48",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数是模型优化的目标，用于在众多的参数取值中，识别最理想的参数取值。\n",
    "损失函数的计算在训练过程的代码中，每一轮模型训练的过程都相同，分如下三步：\n",
    "1. 先根据输入数据正向计算预测输出；\n",
    "2. 再根据预测值和真实值计算损失；\n",
    "3. 最后根据损失反向传播梯度并更新参数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94011e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"交叉熵的代码实现\n",
    "\n",
    "在读取数据部分，将标签的类型设置为int，体现它是一个标签而不是实数值（飞浆默认将标签处理成int64）\n",
    "在网络定义部分，将输出层改成“输出10个标签的概率”的模式\n",
    "在训练过程部分，将损失函数从均方误差(常用于回归问题)换成交叉熵（常用于分类问题）\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea9bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个加载数据的迭代器\n",
    "class MnistDataset(paddle.io.Dataset):\n",
    "    def __init__(self, mode):\n",
    "        # 读取原始数据\n",
    "        datafile = '../datasets/mnist.json.gz'\n",
    "        data = json.load(gzip.open(datafile))\n",
    "        \n",
    "        # 划分训练集、验证集和测试集\n",
    "        train_set, valid_set, test_set = data\n",
    "        \n",
    "        # 数据集相关参数：图片宽、高度\n",
    "        self.IMG_ROWS = 28\n",
    "        self.IMG_COLS = 28\n",
    "        \n",
    "        # 拆分数据和标签\n",
    "        if mode=='train':\n",
    "            imgs, labels = train_set[0], train_set[1]\n",
    "        elif mode=='valid':\n",
    "            imgs, labels = valid_set[0], valid_set[1]\n",
    "        elif mode=='test':\n",
    "            imgs, labels = test_set[0], test_set[1]\n",
    "        else:\n",
    "            raise Exception(\"Mode can only be one of ['train', 'valid', 'test']\")\n",
    "                            \n",
    "        # 校验数据\n",
    "        imgs_length = len(imgs)\n",
    "        assert len(imgs) == len(labels), \\\n",
    "            'Length of imgs({}) should be the same as labels({})'.format(len(imgs), len(labels))\n",
    "                            \n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = np.reshape(self.imgs[idx], [1, self.IMG_ROWS, self.IMG_COLS]).astype('float32')\n",
    "        label = np.reshape(self.labels[idx], [1]).astype('int64')\n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0c3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练模式，迭代器每次迭代只返回batch=1的数据\n",
    "train_dataset = MnistDataset(mode='train')\n",
    "\n",
    "# 使用paddle.io.DataLoader，返回的是一个批次数据迭代器，并且是异步的\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=100, shuffle=True, drop_last=True)\n",
    "\n",
    "# 加载验证数据集\n",
    "valid_dataset = MnistDataset(mode='valid')\n",
    "valid_loader = paddle.io.DataLoader(valid_dataset, batch_size=128, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6db0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义多层卷积神经网络\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义一层全连接层，输出维度是10\n",
    "        self.fc = Linear(in_features=980, out_features=10)\n",
    "        \n",
    "    # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\n",
    "    # 卷积层激活函数使用Relu，全连接层激活函数使用softmax\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], 980])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597f80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改计算损失函数\n",
    "def evaluation(model, datasets):\n",
    "    model.eval()\n",
    "    \n",
    "    acc_set = list()\n",
    "    for batch_id, data in enumerate(datasets()):\n",
    "        images, labels = data\n",
    "        images = paddle.to_tensor(images)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        pred = model(images)\n",
    "        acc = paddle.metric.accuracy(input=pred, label=labels)\n",
    "        acc_set.extend(acc.numpy())\n",
    "    \n",
    "    # 计算多个批次的准确率\n",
    "    acc_val_mean = np.array(acc_set).mean()\n",
    "    return acc_val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63df37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change loss function from F.square_error_cost to F.cross_entropy\n",
    "def train(model):\n",
    "    model.train()\n",
    "    \n",
    "    # 调用加载数据的函数\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            # 准备数据\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            # 前向计算的过程\n",
    "            predicts = model(images)\n",
    "            \n",
    "            # 计算损失，使用交叉熵损失函数，取一个批次样本损失的平均值\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            # 每训练了200批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "            \n",
    "            # 后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            # 最小化loss,更新参数\n",
    "            opt.step()\n",
    "            # 清除梯度\n",
    "            opt.clear_grad()\n",
    "            \n",
    "        acc_train_mean = evaluation(model, train_loader)\n",
    "        acc_valid_mean = evaluation(model, valid_loader)\n",
    "        print('train_acc: {}, valid acc: {}'.format(acc_train_mean, acc_valid_mean))\n",
    "        \n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), '../models/mnist.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec43c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.7077954]\n",
      "epoch: 0, batch: 200, loss is: [0.20742299]\n",
      "epoch: 0, batch: 400, loss is: [0.26519164]\n",
      "train_acc: 0.9392999410629272, valid acc: 0.9464142918586731\n",
      "epoch: 1, batch: 0, loss is: [0.21583277]\n",
      "epoch: 1, batch: 200, loss is: [0.3002791]\n",
      "epoch: 1, batch: 400, loss is: [0.06760629]\n",
      "train_acc: 0.9623000025749207, valid acc: 0.9683493375778198\n",
      "epoch: 2, batch: 0, loss is: [0.1559651]\n",
      "epoch: 2, batch: 200, loss is: [0.10081352]\n",
      "epoch: 2, batch: 400, loss is: [0.06157354]\n",
      "train_acc: 0.9690800309181213, valid acc: 0.9719551205635071\n",
      "epoch: 3, batch: 0, loss is: [0.1323459]\n",
      "epoch: 3, batch: 200, loss is: [0.17427988]\n",
      "epoch: 3, batch: 400, loss is: [0.04494239]\n",
      "train_acc: 0.97107994556427, valid acc: 0.9741586446762085\n",
      "epoch: 4, batch: 0, loss is: [0.12689942]\n",
      "epoch: 4, batch: 200, loss is: [0.03878341]\n",
      "epoch: 4, batch: 400, loss is: [0.14332902]\n",
      "train_acc: 0.976099967956543, valid acc: 0.9777644276618958\n",
      "epoch: 5, batch: 0, loss is: [0.05786611]\n",
      "epoch: 5, batch: 200, loss is: [0.11549574]\n",
      "epoch: 5, batch: 400, loss is: [0.12161531]\n",
      "train_acc: 0.9784399271011353, valid acc: 0.977463960647583\n",
      "epoch: 6, batch: 0, loss is: [0.03373885]\n",
      "epoch: 6, batch: 200, loss is: [0.0609744]\n",
      "epoch: 6, batch: 400, loss is: [0.13267756]\n",
      "train_acc: 0.9806799292564392, valid acc: 0.979567289352417\n",
      "epoch: 7, batch: 0, loss is: [0.06979278]\n",
      "epoch: 7, batch: 200, loss is: [0.04083593]\n",
      "epoch: 7, batch: 400, loss is: [0.03264646]\n",
      "train_acc: 0.9822999835014343, valid acc: 0.9805688858032227\n",
      "epoch: 8, batch: 0, loss is: [0.10596579]\n",
      "epoch: 8, batch: 200, loss is: [0.02567289]\n",
      "epoch: 8, batch: 400, loss is: [0.12517647]\n",
      "train_acc: 0.9838199615478516, valid acc: 0.9815705418586731\n",
      "epoch: 9, batch: 0, loss is: [0.02572048]\n",
      "epoch: 9, batch: 200, loss is: [0.04029049]\n",
      "epoch: 9, batch: 400, loss is: [0.07855061]\n",
      "train_acc: 0.9834198951721191, valid acc: 0.9816706776618958\n"
     ]
    }
   ],
   "source": [
    "# Init Model\n",
    "model = MNIST()\n",
    "\n",
    "# Start training\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aa9eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local image, and convert format to model\n",
    "def load_image(img_path):\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((28, 28))\n",
    "    img = np.array(img).reshape(1, 1, 28, 28).astype(np.float32)\n",
    "    # 图像归一化\n",
    "    img = 1.0 - img / 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cbcb3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别结果： 6\n"
     ]
    }
   ],
   "source": [
    "model = MNIST()\n",
    "model_params_path = '../models/mnist.pdparams'\n",
    "img_path = '../datasets/mnist_example_6.jpg'\n",
    "\n",
    "# Load model's params\n",
    "param_dict = paddle.load(model_params_path)\n",
    "model.load_dict(param_dict)\n",
    "\n",
    "model.eval()\n",
    "tensor_img = load_image(img_path)\n",
    "\n",
    "# 模型返回10个分类标签的对应概率\n",
    "results = model(paddle.to_tensor(tensor_img))\n",
    "\n",
    "# 取出概率最大的标签作为预测输出\n",
    "label = np.argsort(results.numpy())\n",
    "\n",
    "print('识别结果：', label[0][-1])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
