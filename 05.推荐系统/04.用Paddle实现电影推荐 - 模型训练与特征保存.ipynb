{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be862af8",
   "metadata": {},
   "source": [
    "## 数据处理与读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68e8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class DataProcessor():\n",
    "    # 初始化\n",
    "    def __init__(self, use_poster=False):\n",
    "        self.use_poster = use_poster\n",
    "        \n",
    "        # 声明数据文件路径\n",
    "        user_info_path = '../datasets/ml-1m/users.dat'\n",
    "        movie_info_path = '../datasets/ml-1m/movies.dat'\n",
    "        if use_poster:\n",
    "            rating_info_path = '../datasets/ml-1m/new_rating.txt'\n",
    "        else:\n",
    "            rating_info_path = '../datasets/ml-1m/ratings.dat'\n",
    "        self.post_path = '../datasets/ml-1m/posters/'\n",
    "        \n",
    "        # 记录用户数据的最大ID\n",
    "        self.max_user_id = 0\n",
    "        self.max_user_age = 0\n",
    "        self.max_user_job = 0\n",
    "        \n",
    "        # 获取用户数据\n",
    "        self.user_info = self.get_user_info(user_info_path)\n",
    "        \n",
    "        # 获取电影数据\n",
    "        self.movie_info, self.movie_titles, self.movie_cats = self.get_movie_info(movie_info_path)\n",
    "        \n",
    "        # 记录电影的最大ID\n",
    "        self.max_movie_id = np.max(list(map(int, self.movie_info.keys())))\n",
    "        self.max_movie_title = np.max([self.movie_titles[k] for k in self.movie_titles])\n",
    "        self.max_movie_cat = np.max([self.movie_cats[k] for k in self.movie_cats])\n",
    "        \n",
    "        # 获取评分数据\n",
    "        self.rating_info = self.get_rating_info(rating_info_path)\n",
    "        \n",
    "        # 构建数据集\n",
    "        self.dataset = self.get_dataset(user_info=self.user_info, \n",
    "                                       movie_info=self.movie_info, rating_info=self.rating_info)\n",
    "        \n",
    "        # 划分数据集\n",
    "        self.train_dataset = self.dataset[:int(len(self.dataset) * 0.9)]\n",
    "        self.test_dataset = self.dataset[int(len(self.dataset) * 0.9):]\n",
    "        \n",
    "        # 打印测试\n",
    "        print('用户数据量：{}，电影数据量：{}'.format(len(self.user_info), len(self.movie_info)))\n",
    "        print('构建的数据集总量：{}，其中训练集：{}，测试集：{}'.format(len(self.dataset), \n",
    "                                                   len(self.train_dataset), len(self.test_dataset)))\n",
    "        \n",
    "    # 获取用户数据\n",
    "    def get_user_info(self, path):\n",
    "        def gender2num(gender):\n",
    "            return 1 if gender == 'F' else 0\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        user_info = {}\n",
    "        \n",
    "        for item in data:\n",
    "            item = item.strip().split('::')\n",
    "            user_id = item[0]\n",
    "            user_info[user_id] = {\n",
    "                'user_id': int(user_id),\n",
    "                'gender': gender2num(item[1]),\n",
    "                'age': int(item[2]),\n",
    "                'job': int(item[3])\n",
    "            }\n",
    "            self.max_user_id = max(self.max_user_id, int(user_id))\n",
    "            self.max_user_age = max(self.max_user_age, int(item[2]))\n",
    "            self.max_user_job = max(self.max_user_job, int(item[3]))\n",
    "\n",
    "        return user_info\n",
    "    \n",
    "    # 获取电影数据\n",
    "    def get_movie_info(self, path):\n",
    "        with open(path, 'r', encoding='ISO-8859-1') as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        # 建立3个字典，分别存放电影的所有、名称、类别信息\n",
    "        movie_info, movie_titles, movie_cats = {}, {}, {}\n",
    "\n",
    "        # 对电影名称、类别中不同的单词计数\n",
    "        t_count, c_count = 1, 1\n",
    "\n",
    "        # 按行读取数据并处理\n",
    "        for item in data:\n",
    "            item = item.strip().split('::')\n",
    "            v_id = item[0]\n",
    "            v_title = item[1][:-7]  # 去除title里的上映年份\n",
    "            v_year = item[1][-5:-1] # 获取上映年份\n",
    "            v_cat = item[2].split('|')\n",
    "\n",
    "            # 统计电影名称包含的单词，并给每个单词一个序号，存放在movie_titles中\n",
    "            titles = v_title.split()\n",
    "            for t in titles:\n",
    "                if t not in movie_titles:\n",
    "                    movie_titles[t] = t_count\n",
    "                    t_count += 1\n",
    "\n",
    "            # 统计电影类别包含的单词，并给每个单词一个序号，存放在movie_cat中\n",
    "            for c in v_cat:\n",
    "                if c not in movie_cats:\n",
    "                    movie_cats[c] = c_count\n",
    "                    c_count += 1\n",
    "\n",
    "            # 补0使电影名称对应的列表长度为15（最长的电影名称长度为15）\n",
    "            title = [movie_titles[k] for k in titles]\n",
    "            while len(title) < 15:\n",
    "                title.append(0)\n",
    "\n",
    "            # 补0使电影类别对应的列表长度为6（最多类别为6）\n",
    "            cat = [movie_cats[k] for k in v_cat]\n",
    "            while len(cat) < 6:\n",
    "                cat.append(0)\n",
    "\n",
    "            # 保存电影完整信息\n",
    "            movie_info[v_id] = {\n",
    "                'movie_id': int(v_id),\n",
    "                'title': title,\n",
    "                'cat': cat,\n",
    "                'year': int(v_year)\n",
    "            }\n",
    "\n",
    "        return movie_info, movie_titles, movie_cats\n",
    "    \n",
    "    # 获取评分数据\n",
    "    def get_rating_info(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        rating_info = {}\n",
    "\n",
    "        for item in data:\n",
    "            item = item.strip().split('::')\n",
    "            user_id, movie_id, score = item[0], item[1], item[2]\n",
    "            if user_id not in rating_info.keys():\n",
    "                rating_info[user_id] = {movie_id: float(score)}\n",
    "            else:\n",
    "                rating_info[user_id][movie_id] = float(score)\n",
    "\n",
    "        return rating_info\n",
    "    \n",
    "    # 构建数据集\n",
    "    def get_dataset(self, user_info, movie_info, rating_info):\n",
    "        dataset = []\n",
    "\n",
    "        # 按照评分数据的key值索引数据\n",
    "        for user_id in rating_info.keys():\n",
    "            user_ratings = rating_info[user_id]\n",
    "            for movie_id in user_ratings:\n",
    "                dataset.append({\n",
    "                    'user_info': user_info[user_id],\n",
    "                    'movie_info': movie_info[movie_id],\n",
    "                    'score': user_ratings[movie_id]\n",
    "                })\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    # 数据加载器\n",
    "    def load_data(self, dataset=None, mode='train'):\n",
    "        BATCHSIZE = 256  # 定义批次大小\n",
    "        data_length = len(dataset)\n",
    "        index_list = list(range(data_length))\n",
    "\n",
    "        # 定义数据迭代加载器\n",
    "        def data_generator():\n",
    "            # 训练模式下，打乱训练数据\n",
    "            if mode == 'train':\n",
    "                random.shuffle(index_list)\n",
    "\n",
    "            # 声明每个特征的列表\n",
    "            user_id_list, user_gender_list, user_age_list, user_job_list = [], [], [], []\n",
    "            movie_id_list, movie_title_list, movie_cat_list, movie_poster_list = [], [], [], []\n",
    "            score_list = []\n",
    "\n",
    "            # 按索引遍历输入数据集\n",
    "            for idx, i in enumerate(index_list):\n",
    "                # 获取特征数据并保存到对应特征列表中\n",
    "                user_id_list.append(dataset[i]['user_info']['user_id'])\n",
    "                user_gender_list.append(dataset[i]['user_info']['gender'])\n",
    "                user_age_list.append(dataset[i]['user_info']['age'])\n",
    "                user_job_list.append(dataset[i]['user_info']['job'])\n",
    "\n",
    "                movie_id_list.append(dataset[i]['movie_info']['movie_id'])\n",
    "                movie_title_list.append(dataset[i]['movie_info']['title'])\n",
    "                movie_cat_list.append(dataset[i]['movie_info']['cat'])\n",
    "\n",
    "                # 如果使用电影海报数据\n",
    "                if self.use_poster:\n",
    "                    movie_id = dataset[i]['movie_info']['movie_id']\n",
    "                    poster = Image.open(poster_path + 'mov_id{}.jpg'.format(str(movie_id)))\n",
    "                    poster = poster.resize([64, 64])\n",
    "                    if len(poster.size) <= 2:\n",
    "                        poster = poster.convert('RGB')\n",
    "\n",
    "                    movie_poster_list.append(np.array(poster))\n",
    "\n",
    "                score_list.append(int(dataset[i]['score']))\n",
    "\n",
    "                # 如果读取到数据量达到定义的批次大小，则返回当前批次\n",
    "                if len(user_id_list) == BATCHSIZE:\n",
    "                    # 转换list到ndarray，并reshape到固定形状\n",
    "                    user_id_arr = np.array(user_id_list)\n",
    "                    user_gender_arr = np.array(user_gender_list)\n",
    "                    user_age_arr = np.array(user_age_list)\n",
    "                    user_job_arr = np.array(user_job_list)\n",
    "\n",
    "                    movie_id_arr = np.array(movie_id_list)\n",
    "                    movie_title_arr = np.reshape(np.array(movie_title_list), [BATCHSIZE, 1, 15]).astype(np.int64)\n",
    "                    movie_cat_arr = np.reshape(np.array(movie_cat_list), [BATCHSIZE, 6]).astype(np.int64)\n",
    "\n",
    "                    if self.use_poster:\n",
    "                        movie_poster_arr = np.reshape(np.array(movie_poster_list)/127.5 - 1, \n",
    "                                                      [BATCHSIZE, 3, 64, 64]).astype(np.float32)\n",
    "                    else:\n",
    "                        movie_poster_arr = np.array([0.])\n",
    "\n",
    "                    score_arr = np.reshape(np.array(score_list), [-1, 1]).astype(np.float32)\n",
    "\n",
    "                    # 返回当前批次数据\n",
    "                    yield [user_id_arr, user_gender_arr, user_age_arr, user_job_arr], \\\n",
    "                            [movie_id_arr, movie_title_arr, movie_cat_arr, movie_poster_arr], score_arr\n",
    "\n",
    "                    # 清空数据\n",
    "                    user_id_list, user_gender_list, user_age_list, user_job_list = [], [], [], []\n",
    "                    movie_id_list, movie_title_list, movie_cat_list, movie_poster_list = [], [], [], []\n",
    "                    score_list = []\n",
    "\n",
    "        return data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db4898",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e2d3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import Linear, Embedding, Conv2D\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "class Model(nn.Layer):\n",
    "    def __init__(self, use_poster, use_mov_title, use_mov_cat, use_age_job, fc_sizes):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 将传入的name信息和bool型参数添加到模型类中\n",
    "        self.use_mov_poster = use_poster\n",
    "        self.use_mov_title = use_mov_title\n",
    "        self.use_usr_age_job = use_age_job\n",
    "        self.use_mov_cat = use_mov_cat\n",
    "        self.fc_sizes = fc_sizes\n",
    "        \n",
    "        # 获取数据集的信息，并构建训练和验证集的数据迭代器\n",
    "        Dataset = DataProcessor(self.use_mov_poster)\n",
    "        self.Dataset = Dataset\n",
    "        self.trainset = self.Dataset.train_dataset\n",
    "        self.valset = self.Dataset.test_dataset\n",
    "        self.train_loader = self.Dataset.load_data(dataset=self.trainset, mode='train')\n",
    "        self.valid_loader = self.Dataset.load_data(dataset=self.valset, mode='valid')\n",
    "\n",
    "        \"\"\" define network layer for embedding usr info \"\"\"\n",
    "        USR_ID_NUM = Dataset.max_user_id + 1\n",
    "        # 对用户ID做映射，并紧接着一个Linear层\n",
    "        self.usr_emb = Embedding(num_embeddings=USR_ID_NUM, embedding_dim=32, sparse=False)\n",
    "        self.usr_fc = Linear(in_features=32, out_features=32)\n",
    "        \n",
    "        # 对用户性别信息做映射，并紧接着一个Linear层\n",
    "        USR_GENDER_DICT_SIZE = 2\n",
    "        self.usr_gender_emb = Embedding(num_embeddings=USR_GENDER_DICT_SIZE, embedding_dim=16)\n",
    "        self.usr_gender_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 对用户年龄信息做映射，并紧接着一个Linear层\n",
    "        USR_AGE_DICT_SIZE = Dataset.max_user_age + 1\n",
    "        self.usr_age_emb = Embedding(num_embeddings=USR_AGE_DICT_SIZE, embedding_dim=16)\n",
    "        self.usr_age_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 对用户职业信息做映射，并紧接着一个Linear层\n",
    "        USR_JOB_DICT_SIZE = Dataset.max_user_job + 1\n",
    "        self.usr_job_emb = Embedding(num_embeddings=USR_JOB_DICT_SIZE, embedding_dim=16)\n",
    "        self.usr_job_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 新建一个Linear层，用于整合用户数据信息\n",
    "        self.usr_combined = Linear(in_features=80, out_features=200)\n",
    "        \n",
    "        \"\"\" define network layer for embedding usr info \"\"\"\n",
    "        # 对电影ID信息做映射，并紧接着一个Linear层\n",
    "        MOV_DICT_SIZE = Dataset.max_movie_id + 1\n",
    "        self.mov_emb = Embedding(num_embeddings=MOV_DICT_SIZE, embedding_dim=32)\n",
    "        self.mov_fc = Linear(in_features=32, out_features=32)\n",
    "        \n",
    "        # 对电影类别做映射\n",
    "        CATEGORY_DICT_SIZE = len(Dataset.movie_cats) + 1\n",
    "        self.mov_cat_emb = Embedding(num_embeddings=CATEGORY_DICT_SIZE, embedding_dim=32, sparse=False)\n",
    "        self.mov_cat_fc = Linear(in_features=32, out_features=32)\n",
    "        \n",
    "        # 对电影名称做映射\n",
    "        MOV_TITLE_DICT_SIZE = len(Dataset.movie_titles) + 1\n",
    "        self.mov_title_emb = Embedding(num_embeddings=MOV_TITLE_DICT_SIZE, embedding_dim=32, sparse=False)\n",
    "        self.mov_title_conv = Conv2D(in_channels=1, out_channels=1, kernel_size=(3, 1), stride=(2,1), padding=0)\n",
    "        self.mov_title_conv2 = Conv2D(in_channels=1, out_channels=1, kernel_size=(3, 1), stride=1, padding=0)\n",
    "        \n",
    "        # 新建一个FC层，用于整合电影特征\n",
    "        self.mov_concat_embed = Linear(in_features=96, out_features=200)\n",
    "\n",
    "        user_sizes = [200] + self.fc_sizes\n",
    "        acts = [\"relu\" for _ in range(len(self.fc_sizes))]\n",
    "        self._user_layers = []\n",
    "        for i in range(len(self.fc_sizes)):\n",
    "            linear = Linear(\n",
    "                in_features=user_sizes[i],\n",
    "                out_features=user_sizes[i + 1],\n",
    "                weight_attr=paddle.ParamAttr(\n",
    "                    initializer=nn.initializer.Normal(\n",
    "                        std=1.0 / math.sqrt(user_sizes[i]))))\n",
    "            self.add_sublayer('linear_user_%d' % i, linear)\n",
    "            self._user_layers.append(linear)\n",
    "            if acts[i] == 'relu':\n",
    "                act = nn.ReLU()\n",
    "                self.add_sublayer('user_act_%d' % i, act)\n",
    "                self._user_layers.append(act)\n",
    "                \n",
    "        #电影特征和用户特征使用了不同的全连接层，不共享参数\n",
    "        movie_sizes = [200] + self.fc_sizes\n",
    "        acts = [\"relu\" for _ in range(len(self.fc_sizes))]\n",
    "        self._movie_layers = []\n",
    "        for i in range(len(self.fc_sizes)):\n",
    "            linear = nn.Linear(\n",
    "                in_features=movie_sizes[i],\n",
    "                out_features=movie_sizes[i + 1],\n",
    "                weight_attr=paddle.ParamAttr(\n",
    "                    initializer=nn.initializer.Normal(\n",
    "                        std=1.0 / math.sqrt(movie_sizes[i]))))\n",
    "            self.add_sublayer('linear_movie_%d' % i, linear)\n",
    "            self._movie_layers.append(linear)\n",
    "            if acts[i] == 'relu':\n",
    "                act = nn.ReLU()\n",
    "                self.add_sublayer('movie_act_%d' % i, act)\n",
    "                self._movie_layers.append(act)\n",
    "                \n",
    "    # 定义计算用户特征的前向运算过程\n",
    "    def get_usr_feat(self, usr_var):\n",
    "        \"\"\" get usr features\"\"\"\n",
    "        # 获取到用户数据\n",
    "        usr_id, usr_gender, usr_age, usr_job = usr_var\n",
    "        # 将用户的ID数据经过embedding和Linear计算，得到的特征保存在feats_collect中\n",
    "        feats_collect = []\n",
    "        usr_id = self.usr_emb(usr_id)\n",
    "        usr_id = self.usr_fc(usr_id)\n",
    "        usr_id = F.relu(usr_id)\n",
    "        feats_collect.append(usr_id)\n",
    "        \n",
    "        # 计算用户的性别特征，并保存在feats_collect中\n",
    "        usr_gender = self.usr_gender_emb(usr_gender)\n",
    "        usr_gender = self.usr_gender_fc(usr_gender)\n",
    "        usr_gender = F.relu(usr_gender)\n",
    "        feats_collect.append(usr_gender)\n",
    "        # 选择是否使用用户的年龄-职业特征\n",
    "        if self.use_usr_age_job:\n",
    "            # 计算用户的年龄特征，并保存在feats_collect中\n",
    "            usr_age = self.usr_age_emb(usr_age)\n",
    "            usr_age = self.usr_age_fc(usr_age)\n",
    "            usr_age = F.relu(usr_age)\n",
    "            feats_collect.append(usr_age)\n",
    "            # 计算用户的职业特征，并保存在feats_collect中\n",
    "            usr_job = self.usr_job_emb(usr_job)\n",
    "            usr_job = self.usr_job_fc(usr_job)\n",
    "            usr_job = F.relu(usr_job)\n",
    "            feats_collect.append(usr_job)\n",
    "        \n",
    "        # 将用户的特征级联，并通过Linear层得到最终的用户特征\n",
    "        usr_feat = paddle.concat(feats_collect, axis=1)\n",
    "        user_features = F.tanh(self.usr_combined(usr_feat))\n",
    "        #通过3层全链接层，获得用于计算相似度的用户特征和电影特征\n",
    "        for n_layer in self._user_layers:\n",
    "            user_features = n_layer(user_features)\n",
    "\n",
    "        return user_features\n",
    "    \n",
    "    # 定义电影特征的前向计算过程\n",
    "    def get_mov_feat(self, mov_var):\n",
    "        \"\"\" get movie features\"\"\"\n",
    "        # 获得电影数据\n",
    "        mov_id, mov_title, mov_cat, mov_poster = mov_var\n",
    "        feats_collect = []\n",
    "        # 获得batchsize的大小\n",
    "        batch_size = mov_id.shape[0]\n",
    "        # 计算电影ID的特征，并存在feats_collect中\n",
    "        mov_id = self.mov_emb(mov_id)\n",
    "        mov_id = self.mov_fc(mov_id)\n",
    "        mov_id = F.relu(mov_id)\n",
    "        feats_collect.append(mov_id)\n",
    "        \n",
    "        # 如果使用电影的种类数据，计算电影种类特征的映射\n",
    "        if self.use_mov_cat:\n",
    "            # 计算电影种类的特征映射，对多个种类的特征求和得到最终特征\n",
    "            mov_cat = self.mov_cat_emb(mov_cat)\n",
    "            mov_cat = paddle.sum(mov_cat, axis=1, keepdim=False)\n",
    "\n",
    "            mov_cat = self.mov_cat_fc(mov_cat)\n",
    "            feats_collect.append(mov_cat)\n",
    "\n",
    "        if self.use_mov_title:\n",
    "            # 计算电影名字的特征映射，对特征映射使用卷积计算最终的特征\n",
    "            mov_title = self.mov_title_emb(mov_title)\n",
    "            mov_title = F.relu(self.mov_title_conv2(F.relu(self.mov_title_conv(mov_title))))\n",
    "            mov_title = paddle.sum(mov_title, axis=2, keepdim=False)\n",
    "            mov_title = F.relu(mov_title)\n",
    "            mov_title = paddle.reshape(mov_title, [batch_size, -1])\n",
    "            feats_collect.append(mov_title)\n",
    "            \n",
    "        # 使用一个全连接层，整合所有电影特征，映射为一个200维的特征向量\n",
    "        mov_feat = paddle.concat(feats_collect, axis=1)\n",
    "        mov_features = F.tanh(self.mov_concat_embed(mov_feat))\n",
    "\n",
    "        for n_layer in self._movie_layers:\n",
    "            mov_features = n_layer(mov_features)\n",
    "\n",
    "        return mov_features\n",
    "    \n",
    "    # 定义个性化推荐算法的前向计算\n",
    "    def forward(self, usr_var, mov_var):\n",
    "        # 计算用户特征和电影特征\n",
    "        usr_feat = self.get_usr_feat(usr_var)\n",
    "        mov_feat = self.get_mov_feat(mov_var)\n",
    "\n",
    "        #通过3层全连接层，获得用于计算相似度的用户特征和电影特征\n",
    "#         for n_layer in self._user_layers:\n",
    "#             user_features = n_layer(user_features)\n",
    "\n",
    "#         for n_layer in self._movie_layers:\n",
    "#             mov_features = n_layer(mov_features)\n",
    "\n",
    "        # 根据计算的特征计算相似度\n",
    "        res = F.cosine_similarity(usr_feat, mov_feat)\n",
    "        \n",
    "        # 将相似度扩大范围到和电影评分相同数据范围\n",
    "        res = paddle.scale(res, scale=5)\n",
    "        \n",
    "        return usr_feat, mov_feat, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97488b5a",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbd60f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # 配置训练参数\n",
    "    paddle.set_device('cpu')\n",
    "    lr = 0.001  # 学习率\n",
    "    epoches = 10  # 训练轮次\n",
    "    \n",
    "    # 启动训练\n",
    "    model.train()\n",
    "    \n",
    "    # 加载数据读取器\n",
    "    data_loader = model.train_loader\n",
    "    \n",
    "    # 使用adam优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "    \n",
    "    for epoch in range(0, epoches):\n",
    "        for idx, data in enumerate(data_loader()):\n",
    "            # 获得数据，并转为tensor格式\n",
    "            usr, mov, score = data\n",
    "            usr_v = [paddle.to_tensor(var) for var in usr]\n",
    "            mov_v = [paddle.to_tensor(var) for var in mov]\n",
    "            scores_label = paddle.to_tensor(score)\n",
    "            # 计算出算法的前向计算结果\n",
    "            usr_feat, mov_feat, scores_predict = model(usr_v, mov_v)\n",
    "            # 计算loss\n",
    "            loss = F.square_error_cost(scores_predict, scores_label)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            if idx % 500 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, idx, avg_loss.numpy()))\n",
    "                \n",
    "            # 损失函数下降，并清除梯度\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # 每个epoch 保存一次模型\n",
    "        paddle.save(model.state_dict(), '../models/epoch'+str(epoch)+'.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a80fdb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数据量：6040，电影数据量：3883\n",
      "构建的数据集总量：1000209，其中训练集：900188，测试集：100021\n",
      "epoch: 0, batch_id: 0, loss is: [4.898512]\n",
      "epoch: 0, batch_id: 500, loss is: [1.1242074]\n",
      "epoch: 0, batch_id: 1000, loss is: [1.1168172]\n",
      "epoch: 0, batch_id: 1500, loss is: [1.3267958]\n",
      "epoch: 0, batch_id: 2000, loss is: [1.4226235]\n",
      "epoch: 0, batch_id: 2500, loss is: [1.2343428]\n",
      "epoch: 0, batch_id: 3000, loss is: [1.2633283]\n",
      "epoch: 0, batch_id: 3500, loss is: [1.3778837]\n",
      "epoch: 1, batch_id: 0, loss is: [1.2331593]\n",
      "epoch: 1, batch_id: 500, loss is: [1.2399149]\n",
      "epoch: 1, batch_id: 1000, loss is: [1.1923554]\n",
      "epoch: 1, batch_id: 1500, loss is: [1.2333798]\n",
      "epoch: 1, batch_id: 2000, loss is: [1.3206844]\n",
      "epoch: 1, batch_id: 2500, loss is: [1.3114982]\n",
      "epoch: 1, batch_id: 3000, loss is: [1.0118706]\n",
      "epoch: 1, batch_id: 3500, loss is: [1.1784894]\n",
      "epoch: 2, batch_id: 0, loss is: [1.2836294]\n",
      "epoch: 2, batch_id: 500, loss is: [1.2054243]\n",
      "epoch: 2, batch_id: 1000, loss is: [1.1820899]\n",
      "epoch: 2, batch_id: 1500, loss is: [1.1381238]\n",
      "epoch: 2, batch_id: 2000, loss is: [1.321756]\n",
      "epoch: 2, batch_id: 2500, loss is: [1.1951689]\n",
      "epoch: 2, batch_id: 3000, loss is: [1.4320664]\n",
      "epoch: 2, batch_id: 3500, loss is: [1.2879142]\n",
      "epoch: 3, batch_id: 0, loss is: [1.1903032]\n",
      "epoch: 3, batch_id: 500, loss is: [1.302428]\n",
      "epoch: 3, batch_id: 1000, loss is: [1.1905711]\n",
      "epoch: 3, batch_id: 1500, loss is: [1.451607]\n",
      "epoch: 3, batch_id: 2000, loss is: [1.3080058]\n",
      "epoch: 3, batch_id: 2500, loss is: [1.174892]\n",
      "epoch: 3, batch_id: 3000, loss is: [1.1768622]\n",
      "epoch: 3, batch_id: 3500, loss is: [1.1175468]\n",
      "epoch: 4, batch_id: 0, loss is: [1.2269733]\n",
      "epoch: 4, batch_id: 500, loss is: [1.3664497]\n",
      "epoch: 4, batch_id: 1000, loss is: [1.2887162]\n",
      "epoch: 4, batch_id: 1500, loss is: [1.3635802]\n",
      "epoch: 4, batch_id: 2000, loss is: [1.2395405]\n",
      "epoch: 4, batch_id: 2500, loss is: [1.3124166]\n",
      "epoch: 4, batch_id: 3000, loss is: [1.4183967]\n",
      "epoch: 4, batch_id: 3500, loss is: [1.3803439]\n",
      "epoch: 5, batch_id: 0, loss is: [1.1370556]\n",
      "epoch: 5, batch_id: 500, loss is: [1.3322833]\n",
      "epoch: 5, batch_id: 1000, loss is: [1.2692143]\n",
      "epoch: 5, batch_id: 1500, loss is: [1.2105281]\n",
      "epoch: 5, batch_id: 2000, loss is: [1.260233]\n",
      "epoch: 5, batch_id: 2500, loss is: [1.2482221]\n",
      "epoch: 5, batch_id: 3000, loss is: [1.3234594]\n",
      "epoch: 5, batch_id: 3500, loss is: [1.2711656]\n",
      "epoch: 6, batch_id: 0, loss is: [1.265051]\n",
      "epoch: 6, batch_id: 500, loss is: [1.1909165]\n",
      "epoch: 6, batch_id: 1000, loss is: [1.3057956]\n",
      "epoch: 6, batch_id: 1500, loss is: [1.1204711]\n",
      "epoch: 6, batch_id: 2000, loss is: [1.0655675]\n",
      "epoch: 6, batch_id: 2500, loss is: [1.2127421]\n",
      "epoch: 6, batch_id: 3000, loss is: [1.2795336]\n",
      "epoch: 6, batch_id: 3500, loss is: [1.2118974]\n",
      "epoch: 7, batch_id: 0, loss is: [1.2847626]\n",
      "epoch: 7, batch_id: 500, loss is: [1.2398134]\n",
      "epoch: 7, batch_id: 1000, loss is: [1.2234249]\n",
      "epoch: 7, batch_id: 1500, loss is: [1.1667544]\n",
      "epoch: 7, batch_id: 2000, loss is: [1.2089527]\n",
      "epoch: 7, batch_id: 2500, loss is: [1.326134]\n",
      "epoch: 7, batch_id: 3000, loss is: [1.1551133]\n",
      "epoch: 7, batch_id: 3500, loss is: [1.1209663]\n",
      "epoch: 8, batch_id: 0, loss is: [1.2704422]\n",
      "epoch: 8, batch_id: 500, loss is: [1.2446103]\n",
      "epoch: 8, batch_id: 1000, loss is: [1.3926195]\n",
      "epoch: 8, batch_id: 1500, loss is: [1.2793384]\n",
      "epoch: 8, batch_id: 2000, loss is: [1.4992433]\n",
      "epoch: 8, batch_id: 2500, loss is: [1.2868452]\n",
      "epoch: 8, batch_id: 3000, loss is: [1.2427104]\n",
      "epoch: 8, batch_id: 3500, loss is: [1.2709227]\n",
      "epoch: 9, batch_id: 0, loss is: [1.1996009]\n",
      "epoch: 9, batch_id: 500, loss is: [1.2945752]\n",
      "epoch: 9, batch_id: 1000, loss is: [1.0500387]\n",
      "epoch: 9, batch_id: 1500, loss is: [1.3537358]\n",
      "epoch: 9, batch_id: 2000, loss is: [1.1474615]\n",
      "epoch: 9, batch_id: 2500, loss is: [1.2155412]\n",
      "epoch: 9, batch_id: 3000, loss is: [1.249183]\n",
      "epoch: 9, batch_id: 3500, loss is: [1.1665354]\n"
     ]
    }
   ],
   "source": [
    "# 启动训练\n",
    "fc_sizes = [128, 64, 32]\n",
    "model = Model(use_poster=False, use_mov_title=True, use_mov_cat=True, use_age_job=True, fc_sizes=fc_sizes)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703a49c",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "对训练的模型在验证集上做评估：\n",
    "- 评分预测精度ACC(Accuracy)：将预测的float数字转成整数，计算预测评分和真实评分的匹配度。评分误差在0.5分以内的算正确，否则算错误。\n",
    "- 评分预测误差（Mean Absolut Error）MAE：计算预测评分和真实评分之间的平均绝对误差。\n",
    "- 均方根误差 （Root Mean Squard Error）RMSE：计算预测评分和真实值之间的平均平方误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaf786e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def evaluation(model, params_file_path):\n",
    "    model_state_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(model_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    acc_set = []\n",
    "    avg_loss_set = []\n",
    "    squaredError=[]\n",
    "    for idx, data in enumerate(model.valid_loader()):\n",
    "        usr, mov, score_label = data\n",
    "        usr_v = [paddle.to_tensor(var) for var in usr]\n",
    "        mov_v = [paddle.to_tensor(var) for var in mov]\n",
    "\n",
    "        _, _, scores_predict = model(usr_v, mov_v)\n",
    "\n",
    "        pred_scores = scores_predict.numpy()\n",
    "        \n",
    "        avg_loss_set.append(np.mean(np.abs(pred_scores - score_label)))\n",
    "        squaredError.extend(np.abs(pred_scores - score_label)**2)\n",
    "\n",
    "        diff = np.abs(pred_scores - score_label)\n",
    "        diff[diff>0.5] = 1\n",
    "        acc = 1 - np.mean(diff)\n",
    "        acc_set.append(acc)\n",
    "    RMSE=sqrt(np.sum(squaredError) / len(squaredError))\n",
    "   \n",
    "    return np.mean(acc_set), np.mean(avg_loss_set), RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b3cc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.20668767812924507 MAE: 0.9399596 RMSE: 18.014302766598174\n",
      "ACC: 0.18804715810677944 MAE: 0.94934756 RMSE: 18.04095623194127\n",
      "ACC: 0.20986424195460784 MAE: 0.9393253 RMSE: 18.028467517391515\n",
      "ACC: 0.19535081569965068 MAE: 0.9456856 RMSE: 18.025422196236182\n",
      "ACC: 0.19717271343255655 MAE: 0.945309 RMSE: 18.029201962300935\n",
      "ACC: 0.19697357477285923 MAE: 0.9451507 RMSE: 18.024269162608338\n",
      "ACC: 0.20950672320830516 MAE: 0.9392087 RMSE: 18.018675207961323\n",
      "ACC: 0.20295047668310312 MAE: 0.94234097 RMSE: 18.019419501378692\n",
      "ACC: 0.19949561479764108 MAE: 0.94398254 RMSE: 18.021888401010077\n",
      "ACC: 0.1986533215412727 MAE: 0.9442707 RMSE: 18.02125425623724\n"
     ]
    }
   ],
   "source": [
    "param_path = \"../models/epoch\"\n",
    "for i in range(10):\n",
    "    acc, mae, RMSE = evaluation(model, param_path+str(i)+'.pdparams')\n",
    "    print(\"ACC:\", acc, \"MAE:\", mae,'RMSE:',RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67355f3",
   "metadata": {},
   "source": [
    "## 保存特征\n",
    "\n",
    "模型训练完成后，得到每个用户、电影对应的特征向量，接下来将这些特征向量保存到本地，这样在进行推荐时，不需要使用神经网络重新提取特征，节约时间成本。\n",
    "\n",
    "保存特征的基本流程：\n",
    "1. 加载预训练好的模型参数。\n",
    "2. 输入数据集的数据，提取整个数据集的用户特征和电影特征。注意数据输入到模型前，要先转成内置的Tensor类型并保证尺寸正确。\n",
    "3. 分别得到用户特征向量和电影特征向量，使用Pickle库保存字典形式的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6c0130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以ID为索引，以字典格式存储数据\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def get_usr_mov_features(model, params_file_path, poster_path):\n",
    "    paddle.set_device('cpu') \n",
    "    usr_pkl = {}\n",
    "    mov_pkl = {}\n",
    "    \n",
    "    # 定义将list中每个元素转成tensor的函数\n",
    "    def list2tensor(inputs, shape):\n",
    "        inputs = np.reshape(np.array(inputs).astype(np.int64), shape)\n",
    "        return paddle.to_tensor(inputs)\n",
    "\n",
    "    # 加载模型参数到模型中，设置为验证模式eval（）\n",
    "    model_state_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    # 获得整个数据集的数据\n",
    "    dataset = model.Dataset.dataset\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        # 获得用户数据，电影数据，评分数据  \n",
    "        # 本案例只转换所有在样本中出现过的user和movie，实际中可以使用业务系统中的全量数据\n",
    "        usr_info, mov_info, score = dataset[i]['user_info'], dataset[i]['movie_info'],dataset[i]['score']\n",
    "        usrid = str(usr_info['user_id'])\n",
    "        movid = str(mov_info['movie_id'])\n",
    "\n",
    "        # 获得用户数据，计算得到用户特征，保存在usr_pkl字典中\n",
    "        if usrid not in usr_pkl.keys():\n",
    "            usr_id_v = list2tensor(usr_info['user_id'], [1])\n",
    "            usr_age_v = list2tensor(usr_info['age'], [1])\n",
    "            usr_gender_v = list2tensor(usr_info['gender'], [1])\n",
    "            usr_job_v = list2tensor(usr_info['job'], [1])\n",
    "\n",
    "            usr_in = [usr_id_v, usr_gender_v, usr_age_v, usr_job_v]\n",
    "            usr_feat = model.get_usr_feat(usr_in)\n",
    "\n",
    "            usr_pkl[usrid] = usr_feat.numpy()\n",
    "        \n",
    "        # 获得电影数据，计算得到电影特征，保存在mov_pkl字典中\n",
    "        if movid not in mov_pkl.keys():\n",
    "            mov_id_v = list2tensor(mov_info['movie_id'], [1])\n",
    "            mov_tit_v = list2tensor(mov_info['title'], [1, 1, 15])\n",
    "            mov_cat_v = list2tensor(mov_info['cat'], [1, 6])\n",
    "\n",
    "            mov_in = [mov_id_v, mov_tit_v, mov_cat_v, None]\n",
    "            mov_feat = model.get_mov_feat(mov_in)\n",
    "\n",
    "            mov_pkl[movid] = mov_feat.numpy() \n",
    "            \n",
    "    print(len(usr_pkl.keys()))\n",
    "    print(len(mov_pkl.keys()))\n",
    "    \n",
    "    # 保存特征到本地\n",
    "    pickle.dump(usr_pkl, open('../models/usr_feat.pkl', 'wb'))\n",
    "    pickle.dump(mov_pkl, open('../models/mov_feat.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f9a43b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n",
      "3706\n"
     ]
    }
   ],
   "source": [
    "param_path = '../models/epoch9.pdparams'\n",
    "poster_path = '../datasets/ml-1m/posters/'\n",
    "\n",
    "get_usr_mov_features(model, param_path, poster_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
