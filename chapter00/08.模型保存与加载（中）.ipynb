{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac39214",
   "metadata": {},
   "source": [
    "### 使用高层API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d330ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"保存动态图模型\n",
    "\n",
    "保存动态图模型的两种方式：\n",
    "- 开启训练时调用的paddle.Model.fit函数可自动保存模型，通过它的参数 save_freq可以设置保存动态图模型的频率，\n",
    "    即多少个 epoch 保存一次模型，默认值是 1\n",
    "- 调用 paddle.Model.saveAPI，只需要传入保存的模型文件的前缀，即可保存训练后的模型参数和优化器参数，\n",
    "    保存后的文件后缀名固定为 .pdparams 和.pdopt。\n",
    "\"\"\"\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.vision.transforms as T\n",
    "from paddle.vision.models import LeNet\n",
    "\n",
    "model = paddle.Model(LeNet())\n",
    "optim = paddle.optimizer.SGD(learning_rate=1e-3,\n",
    "    parameters=model.parameters())\n",
    "model.prepare(optim, paddle.nn.CrossEntropyLoss())\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Transpose(),\n",
    "    T.Normalize([127.5], [127.5])\n",
    "])\n",
    "data = paddle.vision.datasets.MNIST(mode='train', transform=transform)\n",
    "\n",
    "#方式一：设置训练过程中保存模型\n",
    "# model.fit(data, epochs=1, batch_size=32, save_freq=1)\n",
    "\n",
    "#方式二：设置训练后保存模型\n",
    "model.save('../models/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2f92ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n",
      "step   10/1875 - loss: 2.7796 - 24ms/step\n",
      "step   20/1875 - loss: 2.3775 - 21ms/step\n",
      "step   30/1875 - loss: 2.3387 - 21ms/step\n",
      "step   40/1875 - loss: 2.2269 - 22ms/step\n",
      "step   50/1875 - loss: 2.1822 - 24ms/step\n",
      "step   60/1875 - loss: 2.3911 - 24ms/step\n",
      "step   70/1875 - loss: 2.1516 - 25ms/step\n",
      "step   80/1875 - loss: 2.1721 - 25ms/step\n",
      "step   90/1875 - loss: 2.0140 - 26ms/step\n",
      "step  100/1875 - loss: 2.1766 - 25ms/step\n",
      "step  110/1875 - loss: 2.1561 - 26ms/step\n",
      "step  120/1875 - loss: 2.0069 - 26ms/step\n",
      "step  130/1875 - loss: 1.9215 - 25ms/step\n",
      "step  140/1875 - loss: 1.6938 - 26ms/step\n",
      "step  150/1875 - loss: 1.8043 - 26ms/step\n",
      "step  160/1875 - loss: 1.6594 - 26ms/step\n",
      "step  170/1875 - loss: 1.6236 - 26ms/step\n",
      "step  180/1875 - loss: 1.7618 - 26ms/step\n",
      "step  190/1875 - loss: 1.7516 - 26ms/step\n",
      "step  200/1875 - loss: 1.5499 - 26ms/step\n",
      "step  210/1875 - loss: 1.7939 - 26ms/step\n",
      "step  220/1875 - loss: 1.7188 - 26ms/step\n",
      "step  230/1875 - loss: 1.4762 - 26ms/step\n",
      "step  240/1875 - loss: 1.3539 - 26ms/step\n",
      "step  250/1875 - loss: 1.3862 - 26ms/step\n",
      "step  260/1875 - loss: 1.4206 - 26ms/step\n",
      "step  270/1875 - loss: 1.5052 - 26ms/step\n",
      "step  280/1875 - loss: 1.3124 - 26ms/step\n",
      "step  290/1875 - loss: 1.3982 - 27ms/step\n",
      "step  300/1875 - loss: 1.2760 - 27ms/step\n",
      "step  310/1875 - loss: 1.2481 - 27ms/step\n",
      "step  320/1875 - loss: 1.2675 - 26ms/step\n",
      "step  330/1875 - loss: 1.4251 - 26ms/step\n",
      "step  340/1875 - loss: 1.2159 - 26ms/step\n",
      "step  350/1875 - loss: 1.2796 - 26ms/step\n",
      "step  360/1875 - loss: 1.2764 - 25ms/step\n",
      "step  370/1875 - loss: 1.3172 - 25ms/step\n",
      "step  380/1875 - loss: 1.3358 - 25ms/step\n",
      "step  390/1875 - loss: 1.1742 - 25ms/step\n",
      "step  400/1875 - loss: 1.3957 - 25ms/step\n",
      "step  410/1875 - loss: 1.3188 - 24ms/step\n",
      "step  420/1875 - loss: 0.8755 - 24ms/step\n",
      "step  430/1875 - loss: 1.0395 - 24ms/step\n",
      "step  440/1875 - loss: 1.0404 - 24ms/step\n",
      "step  450/1875 - loss: 1.1133 - 24ms/step\n",
      "step  460/1875 - loss: 1.1869 - 24ms/step\n",
      "step  470/1875 - loss: 0.8884 - 24ms/step\n",
      "step  480/1875 - loss: 1.0745 - 24ms/step\n",
      "step  490/1875 - loss: 0.7273 - 23ms/step\n",
      "step  500/1875 - loss: 1.0679 - 23ms/step\n",
      "step  510/1875 - loss: 0.9002 - 23ms/step\n",
      "step  520/1875 - loss: 0.6989 - 23ms/step\n",
      "step  530/1875 - loss: 0.9688 - 23ms/step\n",
      "step  540/1875 - loss: 0.6954 - 23ms/step\n",
      "step  550/1875 - loss: 0.7728 - 23ms/step\n",
      "step  560/1875 - loss: 1.0209 - 23ms/step\n",
      "step  570/1875 - loss: 0.8092 - 22ms/step\n",
      "step  580/1875 - loss: 0.9996 - 22ms/step\n",
      "step  590/1875 - loss: 0.6756 - 22ms/step\n",
      "step  600/1875 - loss: 0.7154 - 22ms/step\n",
      "step  610/1875 - loss: 0.8307 - 22ms/step\n",
      "step  620/1875 - loss: 0.7430 - 22ms/step\n",
      "step  630/1875 - loss: 0.8584 - 22ms/step\n",
      "step  640/1875 - loss: 0.6588 - 22ms/step\n",
      "step  650/1875 - loss: 1.1652 - 22ms/step\n",
      "step  660/1875 - loss: 0.8816 - 22ms/step\n",
      "step  670/1875 - loss: 0.7361 - 22ms/step\n",
      "step  680/1875 - loss: 1.0791 - 22ms/step\n",
      "step  690/1875 - loss: 0.7495 - 22ms/step\n",
      "step  700/1875 - loss: 0.8788 - 22ms/step\n",
      "step  710/1875 - loss: 1.0658 - 22ms/step\n",
      "step  720/1875 - loss: 0.8330 - 21ms/step\n",
      "step  730/1875 - loss: 0.5602 - 21ms/step\n",
      "step  740/1875 - loss: 0.7895 - 21ms/step\n",
      "step  750/1875 - loss: 0.5807 - 21ms/step\n",
      "step  760/1875 - loss: 0.7749 - 21ms/step\n",
      "step  770/1875 - loss: 0.7602 - 21ms/step\n",
      "step  780/1875 - loss: 0.6176 - 21ms/step\n",
      "step  790/1875 - loss: 0.7158 - 21ms/step\n",
      "step  800/1875 - loss: 0.7864 - 21ms/step\n",
      "step  810/1875 - loss: 0.7151 - 21ms/step\n",
      "step  820/1875 - loss: 0.6078 - 21ms/step\n",
      "step  830/1875 - loss: 0.7259 - 21ms/step\n",
      "step  840/1875 - loss: 0.5824 - 22ms/step\n",
      "step  850/1875 - loss: 0.4740 - 22ms/step\n",
      "step  860/1875 - loss: 0.6003 - 22ms/step\n",
      "step  870/1875 - loss: 0.6961 - 22ms/step\n",
      "step  880/1875 - loss: 0.6878 - 22ms/step\n",
      "step  890/1875 - loss: 0.7900 - 22ms/step\n",
      "step  900/1875 - loss: 0.6070 - 23ms/step\n",
      "step  910/1875 - loss: 0.6454 - 23ms/step\n",
      "step  920/1875 - loss: 0.7916 - 23ms/step\n",
      "step  930/1875 - loss: 0.6420 - 23ms/step\n",
      "step  940/1875 - loss: 0.7730 - 23ms/step\n",
      "step  950/1875 - loss: 0.5736 - 23ms/step\n",
      "step  960/1875 - loss: 0.4312 - 23ms/step\n",
      "step  970/1875 - loss: 0.7620 - 23ms/step\n",
      "step  980/1875 - loss: 0.6524 - 23ms/step\n",
      "step  990/1875 - loss: 0.4492 - 23ms/step\n",
      "step 1000/1875 - loss: 0.5378 - 23ms/step\n",
      "step 1010/1875 - loss: 0.5733 - 23ms/step\n",
      "step 1020/1875 - loss: 0.3153 - 23ms/step\n",
      "step 1030/1875 - loss: 0.7715 - 23ms/step\n",
      "step 1040/1875 - loss: 0.4311 - 23ms/step\n",
      "step 1050/1875 - loss: 0.4523 - 23ms/step\n",
      "step 1060/1875 - loss: 0.5400 - 23ms/step\n",
      "step 1070/1875 - loss: 0.6367 - 23ms/step\n",
      "step 1080/1875 - loss: 0.5102 - 23ms/step\n",
      "step 1090/1875 - loss: 0.4934 - 23ms/step\n",
      "step 1100/1875 - loss: 0.5187 - 23ms/step\n",
      "step 1110/1875 - loss: 0.4223 - 23ms/step\n",
      "step 1120/1875 - loss: 0.5031 - 23ms/step\n",
      "step 1130/1875 - loss: 0.5850 - 23ms/step\n",
      "step 1140/1875 - loss: 0.5616 - 23ms/step\n",
      "step 1150/1875 - loss: 0.5118 - 23ms/step\n",
      "step 1160/1875 - loss: 0.2906 - 23ms/step\n",
      "step 1170/1875 - loss: 0.5673 - 23ms/step\n",
      "step 1180/1875 - loss: 0.5210 - 23ms/step\n",
      "step 1190/1875 - loss: 0.4806 - 23ms/step\n",
      "step 1200/1875 - loss: 0.3317 - 23ms/step\n",
      "step 1210/1875 - loss: 0.4840 - 23ms/step\n",
      "step 1220/1875 - loss: 0.3663 - 23ms/step\n",
      "step 1230/1875 - loss: 0.3031 - 23ms/step\n",
      "step 1240/1875 - loss: 0.6147 - 23ms/step\n",
      "step 1250/1875 - loss: 0.5055 - 22ms/step\n",
      "step 1260/1875 - loss: 0.6069 - 22ms/step\n",
      "step 1270/1875 - loss: 0.4606 - 22ms/step\n",
      "step 1280/1875 - loss: 0.3781 - 22ms/step\n",
      "step 1290/1875 - loss: 0.5313 - 22ms/step\n",
      "step 1300/1875 - loss: 0.3609 - 22ms/step\n",
      "step 1310/1875 - loss: 0.6378 - 22ms/step\n",
      "step 1320/1875 - loss: 0.6777 - 22ms/step\n",
      "step 1330/1875 - loss: 0.7035 - 22ms/step\n",
      "step 1340/1875 - loss: 0.4553 - 22ms/step\n",
      "step 1350/1875 - loss: 0.3113 - 22ms/step\n",
      "step 1360/1875 - loss: 0.2241 - 22ms/step\n",
      "step 1370/1875 - loss: 0.6659 - 22ms/step\n",
      "step 1380/1875 - loss: 0.4211 - 22ms/step\n",
      "step 1390/1875 - loss: 0.5376 - 22ms/step\n",
      "step 1400/1875 - loss: 0.4488 - 22ms/step\n",
      "step 1410/1875 - loss: 0.7601 - 22ms/step\n",
      "step 1420/1875 - loss: 0.8653 - 22ms/step\n",
      "step 1430/1875 - loss: 0.4060 - 22ms/step\n",
      "step 1440/1875 - loss: 0.3426 - 22ms/step\n",
      "step 1450/1875 - loss: 0.5067 - 22ms/step\n",
      "step 1460/1875 - loss: 0.3671 - 22ms/step\n",
      "step 1470/1875 - loss: 0.4412 - 22ms/step\n",
      "step 1480/1875 - loss: 0.3085 - 22ms/step\n",
      "step 1490/1875 - loss: 0.5461 - 22ms/step\n",
      "step 1500/1875 - loss: 0.4935 - 21ms/step\n",
      "step 1510/1875 - loss: 0.6242 - 21ms/step\n",
      "step 1520/1875 - loss: 0.4182 - 21ms/step\n",
      "step 1530/1875 - loss: 0.3410 - 21ms/step\n",
      "step 1540/1875 - loss: 0.3645 - 21ms/step\n",
      "step 1550/1875 - loss: 0.6555 - 22ms/step\n",
      "step 1560/1875 - loss: 0.5458 - 22ms/step\n",
      "step 1570/1875 - loss: 0.3446 - 22ms/step\n",
      "step 1580/1875 - loss: 0.4426 - 22ms/step\n",
      "step 1590/1875 - loss: 0.4602 - 22ms/step\n",
      "step 1600/1875 - loss: 0.4587 - 22ms/step\n",
      "step 1610/1875 - loss: 0.3253 - 22ms/step\n",
      "step 1620/1875 - loss: 0.4066 - 22ms/step\n",
      "step 1630/1875 - loss: 0.2305 - 22ms/step\n",
      "step 1640/1875 - loss: 0.4584 - 22ms/step\n",
      "step 1650/1875 - loss: 0.3613 - 22ms/step\n",
      "step 1660/1875 - loss: 0.5387 - 22ms/step\n",
      "step 1670/1875 - loss: 0.5756 - 22ms/step\n",
      "step 1680/1875 - loss: 0.4170 - 22ms/step\n",
      "step 1690/1875 - loss: 0.6002 - 22ms/step\n",
      "step 1700/1875 - loss: 0.3411 - 22ms/step\n",
      "step 1710/1875 - loss: 0.4257 - 22ms/step\n",
      "step 1720/1875 - loss: 0.2769 - 22ms/step\n",
      "step 1730/1875 - loss: 0.2914 - 22ms/step\n",
      "step 1740/1875 - loss: 0.3020 - 22ms/step\n",
      "step 1750/1875 - loss: 0.4524 - 22ms/step\n",
      "step 1760/1875 - loss: 0.2829 - 22ms/step\n",
      "step 1770/1875 - loss: 0.1963 - 22ms/step\n",
      "step 1780/1875 - loss: 0.3643 - 22ms/step\n",
      "step 1790/1875 - loss: 0.2872 - 22ms/step\n",
      "step 1800/1875 - loss: 0.2814 - 22ms/step\n",
      "step 1810/1875 - loss: 0.6013 - 22ms/step\n",
      "step 1820/1875 - loss: 0.3862 - 22ms/step\n",
      "step 1830/1875 - loss: 0.5475 - 22ms/step\n",
      "step 1840/1875 - loss: 0.2844 - 22ms/step\n",
      "step 1850/1875 - loss: 0.4333 - 22ms/step\n",
      "step 1860/1875 - loss: 0.5365 - 22ms/step\n",
      "step 1870/1875 - loss: 0.1954 - 22ms/step\n",
      "step 1875/1875 - loss: 0.2962 - 22ms/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"加载动态图模型\n",
    "\n",
    "高层 API 加载动态图模型所需要调用的 API 是 paddle.Model.load，从指定的文件中载入模型参数和优化器参数（可选）以继续训练。\n",
    "paddle.Model.load需要传入的核心的参数是待加载的模型参数或者优化器参数文件（可选）的前缀（需要保证后缀符合 .pdparams 和.pdopt\n",
    "\"\"\"\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.vision.transforms as T\n",
    "from paddle.vision.models import LeNet\n",
    "\n",
    "model = paddle.Model(LeNet())\n",
    "optim = paddle.optimizer.SGD(learning_rate=1e-3,\n",
    "    parameters=model.parameters())\n",
    "model.prepare(optim, paddle.nn.CrossEntropyLoss())\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Transpose(),\n",
    "    T.Normalize([127.5], [127.5])\n",
    "])\n",
    "data = paddle.vision.datasets.MNIST(mode='train', transform=transform)\n",
    "\n",
    "# 加载模型参数和优化器参数\n",
    "model.load('../models/test')\n",
    "model.fit(data, epochs=1, batch_size=32, save_freq=1)\n",
    "\n",
    "model.save('../models/test_freq') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec55e2b",
   "metadata": {},
   "source": [
    "## 用于推理部署场景\n",
    "\n",
    "由于动态图模型采用 Python 实时执行的方式，开销较大，在性能方面与 C++ 有一定差距；静态图模型将前端 Python 编写的神经网络预定义为 Program 描述，转到 C++ 端重新解析执行，脱离了 Python 依赖，往往执行性能更佳，并且预先拥有完整网络结构也更利于全局优化，在推理部署场景有天然的优势。\n",
    "\n",
    "在飞桨框架中，动态图模型训练完成后，为了在部署场景中获得更好的推理性能，提供了自动将动态图模型保存为静态图模型的功能，主要使用的保存和加载 API 是 paddle.jit.save 和 paddle.jit.load。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bfcba",
   "metadata": {},
   "source": [
    "### 使用高层API\n",
    "\n",
    "高层 API paddle.Model.save可支持保存推理使用的模型，此时高层 API 在动态图下实际上是对paddle.jit.save的封装，在静态图下是对 paddle.static.save_inference_model的封装，会自动将训练好的动态图模型保存为静态图模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daceda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"保存推理使用模型\n",
    "\n",
    "- paddle.Model.save的第一个参数需要设置为待保存的模型和参数等文件的前缀名\n",
    "- 第二个参数 training 表示是否保存动态图模型以继续训练，默认是 True，这里需要设为 False，即保存推理部署所需的参数与文件\n",
    "\n",
    "保存静态图模型会生成三个文件：\n",
    "*.pdmodel         保存模型的网络结构\n",
    "*.pdiparams       保存模型中所有的权重数据\n",
    "*.pdiparams.info  保存和参数状态有关的额外信息\n",
    "\"\"\"\n",
    "\n",
    "model.save('../models/inference_model', False)  # save for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
