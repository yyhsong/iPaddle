{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467902c4",
   "metadata": {},
   "source": [
    "## 文本分类\n",
    "\n",
    "- 文本分类是指使用计算机将文本数据进行自动化归类的任务，是自然语言处理（NLP）中的一项重要任务。\n",
    "- ERNIE是一个预训练模型，使用三种级别的Knowledge Masking帮助模型学习语言知识，在多项任务上超越了BERT。在模型结构方面，它采用了Transformer的Encoder部分作为模型主干进行训练。\n",
    "    - Seq2Seq模型: sequence to sequence模型是一类End-to-End的算法框架，也就是从序列到序列的转换模型框架，应用在机器翻译，自动应答等场景。\n",
    "    - Attention机制（注意力）：注意力机制可以利用人类的认知机制直观解释。例如，我们的视觉系统倾向于关注图像中辅助判断的部分信息，并忽略掉不相关的信息。同样，在自然语言处理的问题中，输入的某些部分可能会比其他部分对决策更有帮助。\n",
    "    - Transfomer模型：很多NLP的语义学习问题涉及到大量的训练数据，而RNN类的模型内部存在计算依赖，无法高效的并行化训练。使用Self-attenion的方法，将RNN变成每个输入与其他输入部分计算匹配度来决定注意力权重的方式，使得模型引入了Attention机制的同时也具备了并行化计算的能力。以这种Self-attention结构为核心，设计Encoder-Decoder的结构形成Transformer模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d8bd7",
   "metadata": {},
   "source": [
    "## 使用开发套件ERNIE实现对新闻标题的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce7c3f",
   "metadata": {},
   "source": [
    "### 数据集介绍\n",
    "\n",
    "THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。\n",
    "\n",
    "本案例使用的数据集是从THUCNews新闻数据中根据新闻类别按照一定的比例提取了新闻标题，其中训练集数据约27.1w，测试集约6.7w条，另有一份记录标签的词表`lable_dict.txt`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc86369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2023-01-11 17:54:31,038 utils.py:148] Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import Dataset\n",
    "\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f20b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"构建数据集类\"\"\"\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        # 加载标签词典\n",
    "        self.label2id = self._load_label_dict(label_path)\n",
    "        \n",
    "        self.label_list = list(self.label2id.keys())\n",
    "        \n",
    "        # 加载数据集\n",
    "        self.data = self._load_data(data_path)\n",
    "        \n",
    "    def _load_data(self, data_path):\n",
    "        dataset = []\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                label, text = line.strip().split('\\t', maxsplit=1)\n",
    "                example = {'text': text, 'label': self.label2id[label]}\n",
    "                dataset.append(example)\n",
    "                \n",
    "        return dataset\n",
    "                \n",
    "    def _load_label_dict(self, label_path):\n",
    "        with open(label_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip().split() for line in f.readlines()]\n",
    "            lines = [(line[0], int(line[1])) for line in lines]\n",
    "            label_dict = dict(lines)\n",
    "            \n",
    "        return label_dict \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51bc2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度： 271167\n",
      "训练集格式： {'text': '爱情测试，你的爱情年老时是啥样', 'label': 0}\n",
      "标签字典： {'星座': 0, '科技': 1, '房产': 2, '股票': 3, '彩票': 4, '时尚': 5, '教育': 6, '体育': 7, '娱乐': 8, '家居': 9, '时政': 10, '社会': 11, '财经': 12, '游戏': 13}\n",
      "标签列表： ['星座', '科技', '房产', '股票', '彩票', '时尚', '教育', '体育', '娱乐', '家居', '时政', '社会', '财经', '游戏']\n"
     ]
    }
   ],
   "source": [
    "data_path = '../datasets/THUCNews/train.txt'\n",
    "label_path = '../datasets/THUCNews/label_dict.txt'\n",
    "\n",
    "news_dataset = NewsDataset(data_path, label_path)\n",
    "\n",
    "print('训练集长度：', news_dataset.__len__())\n",
    "print('训练集格式：', news_dataset.__getitem__(0))\n",
    "\n",
    "print('标签字典：', news_dataset.label2id)\n",
    "print('标签列表：', news_dataset.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0244c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"数据格式转换\n",
    "\n",
    "转换成ERNIE模型需要的语料数据输入格式，输入包括token ids, segment ids(token type ids)和position ids(模型内部生成)。\n",
    "转换数据格式的时候，可以调用PaddleNLP封装好的tokenizer\n",
    "\"\"\"\n",
    "\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    encoded_inputs = tokenizer(text=example['text'], max_seq_length=max_seq_length)\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    \n",
    "    if not is_test:\n",
    "        label = np.array([example['label']], dtype='int64')\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f842f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"构建数据加载器\n",
    "\n",
    "构建DataLoader，将数据组装成规整的mini-batch形式，以便传入模型进行处理，处理流程如下：\n",
    "1. 首先用数据格式转换处理数据为期望的格式，然后构建DataLoader\n",
    "2. 在DataLoader生成mini-batch数据的过程中，通过使用batchify_fn函数进行统一文本序列长度，\n",
    "处理label等操作，以保证返回的数据适合输入到模型中。\n",
    "\"\"\"\n",
    "\n",
    "def create_dataloader(dataset, mode='train', batch_size=1, batchify_fn=None, trans_fn=None):\n",
    "    # 使用该函数将数据处理成模型输入需要的格式\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "        \n",
    "    shuffle = True if mode == 'train' else False\n",
    "    \n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641106d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-01-11 17:54:49,812] [    INFO]\u001b[0m - Already cached /Users/neowong/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-01-11 17:54:49,831] [    INFO]\u001b[0m - tokenizer config file saved in /Users/neowong/.paddlenlp/models/ernie-1.0/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-01-11 17:54:49,832] [    INFO]\u001b[0m - Special tokens file saved in /Users/neowong/.paddlenlp/models/ernie-1.0/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 使用的预训练模型\n",
    "MODEL_NAME = 'ernie-1.0'\n",
    "# 在转换数据格式的时候，调用paddleNLP封装好的tokenizer\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8fc80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规整化mini-batch数据（长短不一）为模型期望的样式\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),       # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack(dtype='int64')  # label\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96104db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Data: \n",
      " [[1 2 3 4]\n",
      " [3 4 5 6]\n",
      " [5 6 7 8]] \n",
      "\n",
      "Padded Data: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]] \n",
      "\n",
      "Ids: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]] \n",
      "\n",
      "Labels: \n",
      " [[1]\n",
      " [0]\n",
      " [1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "# 将多个样本打包成一个批次\n",
    "a = [1, 2, 3, 4]\n",
    "b = [3, 4, 5, 6]\n",
    "c = [5, 6, 7, 8]\n",
    "result = Stack()([a, b, c])\n",
    "print('Stacked Data: \\n', result, '\\n')\n",
    "\n",
    "# 补齐样本长度\n",
    "a = [1, 2, 3, 4]\n",
    "b = [5, 6, 7]\n",
    "c = [8, 9]\n",
    "result = Pad(pad_val=0)([a, b, c])\n",
    "print('Padded Data: \\n', result, '\\n')\n",
    "\n",
    "# 特征集合与标签集合的映射\n",
    "data = [\n",
    "    [[1, 2, 3, 4], [1]],\n",
    "    [[5, 6, 7], [0]],\n",
    "    [[8, 9], [1]]\n",
    "]\n",
    "batchify_fn = Tuple(Pad(pad_val=0), Stack())\n",
    "ids, labels = batchify_fn(data)\n",
    "print('Ids: \\n', ids, '\\n')\n",
    "print('Labels: \\n', labels, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056629f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"模型构建\n",
    "\n",
    "使用PaddleNLP直接加载预训练好的ERNIE模型\n",
    "然后定义用于文本分类的线性层，同时设置使用Dropout的网络优化策略\n",
    "\"\"\"\n",
    "\n",
    "class ErnieForSequenceClassification(paddle.nn.Layer):\n",
    "    def __init__(self, MODEL_NAME, num_class=14, dropout=None):\n",
    "        super(ErnieForSequenceClassification, self).__init__()\n",
    "        \n",
    "        # 通过指定名称加载对应的预训练模型\n",
    "        self.ernie = paddlenlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else self.ernie.config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(self.ernie.config['hidden_size'], num_class)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.ernie(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668e3e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-01-11 17:55:04,675] [    INFO]\u001b[0m - Already cached /Users/neowong/.paddlenlp/models/ernie-tiny/ernie_tiny.pdparams\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"训练配置\"\"\"\n",
    "\n",
    "# 超参设置\n",
    "n_epochs = 1\n",
    "batch_size = 128\n",
    "max_seq_length = 128\n",
    "n_classes = 14\n",
    "dropout_rate = None\n",
    "\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "\n",
    "# 简化版的ERNIE，拥有更快的训练和推理速度\n",
    "MODEL_NAME = 'ernie-tiny'\n",
    "\n",
    "# 加载数据集，构建DataLoader\n",
    "train_set = NewsDataset('../datasets/THUCNews/train.txt', '../datasets/THUCNews/label_dict.txt')\n",
    "test_set = NewsDataset('../datasets/THUCNews/test.txt', '../datasets/THUCNews/label_dict.txt')\n",
    "\n",
    "label2id = train_set.label2id\n",
    "train_set = MapDataset(train_set)\n",
    "test_set = MapDataset(test_set)\n",
    "\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "train_data_loader = create_dataloader(train_set, mode='train', batch_size=batch_size, \n",
    "                                      batchify_fn=batchify_fn, trans_fn=trans_fn)\n",
    "test_data_loader = create_dataloader(test_set, mode='test', batch_size=batch_size,\n",
    "                                     batchify_fn=batchify_fn, trans_fn=trans_fn)\n",
    "\n",
    "# 检测是否可以使用GPU进行训练\n",
    "use_gpu = True if paddle.get_device().startswith('gpu') else False\n",
    "if use_gpu:\n",
    "    paddle.set_device('gpu:0')\n",
    "    \n",
    "# 加载模型\n",
    "model = ErnieForSequenceClassification(MODEL_NAME, num_class=n_classes, dropout=dropout_rate)\n",
    "\n",
    "# 设置优化器\n",
    "num_training_steps = len(train_data_loader) * n_epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3eca50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 2.98997, acc: 0.05859\n",
      "global step 20, epoch: 1, batch: 20, loss: 2.87861, acc: 0.06094\n",
      "global step 30, epoch: 1, batch: 30, loss: 2.62139, acc: 0.06979\n",
      "global step 40, epoch: 1, batch: 40, loss: 2.55458, acc: 0.08398\n",
      "global step 50, epoch: 1, batch: 50, loss: 2.52601, acc: 0.09031\n",
      "global step 60, epoch: 1, batch: 60, loss: 2.40739, acc: 0.10117\n",
      "global step 70, epoch: 1, batch: 70, loss: 2.38055, acc: 0.11161\n",
      "global step 80, epoch: 1, batch: 80, loss: 2.50773, acc: 0.12295\n",
      "global step 90, epoch: 1, batch: 90, loss: 2.19281, acc: 0.13984\n",
      "global step 100, epoch: 1, batch: 100, loss: 2.00311, acc: 0.15906\n",
      "global step 110, epoch: 1, batch: 110, loss: 2.08255, acc: 0.17564\n",
      "global step 120, epoch: 1, batch: 120, loss: 1.67801, acc: 0.19792\n",
      "global step 130, epoch: 1, batch: 130, loss: 1.60479, acc: 0.21689\n",
      "global step 140, epoch: 1, batch: 140, loss: 1.63914, acc: 0.23588\n",
      "global step 150, epoch: 1, batch: 150, loss: 1.51181, acc: 0.25542\n",
      "global step 160, epoch: 1, batch: 160, loss: 1.40100, acc: 0.27349\n",
      "global step 170, epoch: 1, batch: 170, loss: 1.41646, acc: 0.29081\n",
      "global step 180, epoch: 1, batch: 180, loss: 1.18755, acc: 0.30799\n",
      "global step 190, epoch: 1, batch: 190, loss: 1.21691, acc: 0.32418\n",
      "global step 200, epoch: 1, batch: 200, loss: 1.01402, acc: 0.33953\n",
      "global step 210, epoch: 1, batch: 210, loss: 1.03544, acc: 0.35398\n",
      "global step 220, epoch: 1, batch: 220, loss: 1.07879, acc: 0.36786\n",
      "global step 230, epoch: 1, batch: 230, loss: 1.15364, acc: 0.38111\n",
      "global step 240, epoch: 1, batch: 240, loss: 1.04026, acc: 0.39469\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.89019, acc: 0.40622\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.97220, acc: 0.41749\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.96763, acc: 0.42784\n",
      "global step 280, epoch: 1, batch: 280, loss: 1.11151, acc: 0.43725\n",
      "global step 290, epoch: 1, batch: 290, loss: 1.13184, acc: 0.44696\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.73618, acc: 0.45560\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.96833, acc: 0.46371\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.82628, acc: 0.47163\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.84555, acc: 0.48063\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.82633, acc: 0.48826\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.70735, acc: 0.49518\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.70684, acc: 0.50237\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.64241, acc: 0.50921\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.79143, acc: 0.51651\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.72831, acc: 0.52280\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.73396, acc: 0.52828\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.83558, acc: 0.53344\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.61526, acc: 0.53917\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.80378, acc: 0.54413\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.68870, acc: 0.54927\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.86738, acc: 0.55377\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.60396, acc: 0.55853\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.66230, acc: 0.56282\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.71811, acc: 0.56724\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.63282, acc: 0.57170\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.74516, acc: 0.57588\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.76316, acc: 0.57978\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.65679, acc: 0.58340\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.74141, acc: 0.58740\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.55859, acc: 0.59084\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.72907, acc: 0.59439\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.69083, acc: 0.59781\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.64443, acc: 0.60140\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.63649, acc: 0.60459\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.63245, acc: 0.60788\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.39984, acc: 0.61116\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.57739, acc: 0.61422\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.71864, acc: 0.61702\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.68397, acc: 0.61973\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.55011, acc: 0.62279\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.39803, acc: 0.62569\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.56549, acc: 0.62840\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.61303, acc: 0.63087\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.73198, acc: 0.63338\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.58167, acc: 0.63579\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.50970, acc: 0.63828\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.70589, acc: 0.64015\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.49730, acc: 0.64256\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.60547, acc: 0.64485\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.59087, acc: 0.64723\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.47673, acc: 0.64959\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.54938, acc: 0.65173\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.58501, acc: 0.65389\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.45168, acc: 0.65603\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.49945, acc: 0.65813\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.52377, acc: 0.66026\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.39134, acc: 0.66225\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.44359, acc: 0.66403\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.59040, acc: 0.66581\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.58981, acc: 0.66766\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.30996, acc: 0.66956\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.58122, acc: 0.67125\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.61680, acc: 0.67320\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.64944, acc: 0.67478\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.42632, acc: 0.67665\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.52367, acc: 0.67828\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.65651, acc: 0.67980\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.37632, acc: 0.68145\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.54902, acc: 0.68303\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.82118, acc: 0.68441\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.66609, acc: 0.68596\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.61782, acc: 0.68752\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.56436, acc: 0.68894\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.66970, acc: 0.69019\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.44546, acc: 0.69162\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.55592, acc: 0.69307\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.51960, acc: 0.69438\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.52641, acc: 0.69571\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.62311, acc: 0.69702\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.61454, acc: 0.69815\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.48880, acc: 0.69933\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.50839, acc: 0.70049\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.60299, acc: 0.70177\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.74810, acc: 0.70297\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.51814, acc: 0.70421\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.60313, acc: 0.70543\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.53299, acc: 0.70655\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.46496, acc: 0.70770\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.53374, acc: 0.70871\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.31610, acc: 0.70985\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.41131, acc: 0.71103\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.46367, acc: 0.71212\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.44602, acc: 0.71321\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.47547, acc: 0.71417\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.54676, acc: 0.71516\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.56066, acc: 0.71617\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.58073, acc: 0.71723\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.46524, acc: 0.71833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1230, epoch: 1, batch: 1230, loss: 0.53150, acc: 0.71936\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.63918, acc: 0.72029\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.46274, acc: 0.72126\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.54385, acc: 0.72217\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.50451, acc: 0.72320\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.52351, acc: 0.72422\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.40506, acc: 0.72523\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.40808, acc: 0.72604\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.67256, acc: 0.72694\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.58643, acc: 0.72777\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.60177, acc: 0.72874\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.57780, acc: 0.72964\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.62892, acc: 0.73046\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.51418, acc: 0.73134\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.63222, acc: 0.73213\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.40139, acc: 0.73300\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.38604, acc: 0.73376\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.49943, acc: 0.73448\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.51498, acc: 0.73526\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.60970, acc: 0.73600\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.58957, acc: 0.73681\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.59155, acc: 0.73752\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.57368, acc: 0.73811\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.60527, acc: 0.73862\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.52523, acc: 0.73940\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.67609, acc: 0.74009\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.63492, acc: 0.74076\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.52173, acc: 0.74151\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.43741, acc: 0.74222\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.57193, acc: 0.74289\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.55128, acc: 0.74354\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.46320, acc: 0.74422\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.53069, acc: 0.74490\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.43666, acc: 0.74558\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.50334, acc: 0.74619\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.43151, acc: 0.74687\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.38393, acc: 0.74748\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.50929, acc: 0.74809\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.52326, acc: 0.74866\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.71627, acc: 0.74932\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.47789, acc: 0.74995\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.42941, acc: 0.75065\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.39954, acc: 0.75127\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.54620, acc: 0.75182\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.46945, acc: 0.75236\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.41070, acc: 0.75293\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.55775, acc: 0.75352\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.34980, acc: 0.75407\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.38476, acc: 0.75471\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.44411, acc: 0.75522\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.30089, acc: 0.75565\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.43222, acc: 0.75616\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.42281, acc: 0.75684\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.37554, acc: 0.75747\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.53528, acc: 0.75795\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.49625, acc: 0.75835\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.37928, acc: 0.75881\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.67676, acc: 0.75917\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.57799, acc: 0.75964\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.47408, acc: 0.76007\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.51708, acc: 0.76056\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.35818, acc: 0.76103\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.46296, acc: 0.76161\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.47955, acc: 0.76204\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.53744, acc: 0.76258\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.39138, acc: 0.76313\n",
      "global step 1890, epoch: 1, batch: 1890, loss: 0.28055, acc: 0.76368\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.42283, acc: 0.76418\n",
      "global step 1910, epoch: 1, batch: 1910, loss: 0.46737, acc: 0.76459\n",
      "global step 1920, epoch: 1, batch: 1920, loss: 0.34506, acc: 0.76516\n",
      "global step 1930, epoch: 1, batch: 1930, loss: 0.44076, acc: 0.76564\n",
      "global step 1940, epoch: 1, batch: 1940, loss: 0.54084, acc: 0.76609\n",
      "global step 1950, epoch: 1, batch: 1950, loss: 0.57038, acc: 0.76643\n",
      "global step 1960, epoch: 1, batch: 1960, loss: 0.45042, acc: 0.76695\n",
      "global step 1970, epoch: 1, batch: 1970, loss: 0.38544, acc: 0.76740\n",
      "global step 1980, epoch: 1, batch: 1980, loss: 0.50053, acc: 0.76783\n",
      "global step 1990, epoch: 1, batch: 1990, loss: 0.31302, acc: 0.76826\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.43558, acc: 0.76864\n",
      "global step 2010, epoch: 1, batch: 2010, loss: 0.50491, acc: 0.76910\n",
      "global step 2020, epoch: 1, batch: 2020, loss: 0.52402, acc: 0.76952\n",
      "global step 2030, epoch: 1, batch: 2030, loss: 0.39578, acc: 0.76999\n",
      "global step 2040, epoch: 1, batch: 2040, loss: 0.38180, acc: 0.77041\n",
      "global step 2050, epoch: 1, batch: 2050, loss: 0.60817, acc: 0.77079\n",
      "global step 2060, epoch: 1, batch: 2060, loss: 0.46218, acc: 0.77115\n",
      "global step 2070, epoch: 1, batch: 2070, loss: 0.44089, acc: 0.77151\n",
      "global step 2080, epoch: 1, batch: 2080, loss: 0.40590, acc: 0.77190\n",
      "global step 2090, epoch: 1, batch: 2090, loss: 0.40078, acc: 0.77226\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.51530, acc: 0.77267\n",
      "global step 2110, epoch: 1, batch: 2110, loss: 0.44390, acc: 0.77302\n",
      "eval loss: 0.42002, accu: 0.86933\n"
     ]
    }
   ],
   "source": [
    "\"\"\"模型训练与评估\n",
    "\n",
    "在训练过程中，模型会根据Loss不断反向调整模型参数\n",
    "当完成训练后，可以根据模型的评估指标选出训练成功的模型，用于推理\n",
    "\"\"\"\n",
    "\n",
    "# 定义评估指标\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    # 每次使用测试集进行评估时，先重置之前的metric的累积数据\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        # 获取数据\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        \n",
    "        # 执行前向计算\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = F.cross_entropy(input=logits, label=labels)\n",
    "        loss = paddle.mean(loss)\n",
    "        losses.append(loss.numpy())\n",
    "        \n",
    "        # 统计准确率指标\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    \n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    metric.reset()\n",
    "    \n",
    "    \n",
    "def train(model):\n",
    "    global_step=0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            # 获取数据\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            # 模型前向计算\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            loss = F.cross_entropy(input=logits, label=labels)\n",
    "            loss = paddle.mean(loss)\n",
    "\n",
    "            # 统计指标\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "            \n",
    "            # 打印中间训练结果\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 :\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % \n",
    "                      (global_step, epoch, step, loss, acc))\n",
    "            \n",
    "            # 参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        \n",
    "        # 模型评估\n",
    "        evaluate(model, metric, test_data_loader)\n",
    "        \n",
    "        \n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a995c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-01-11 17:55:18,607] [    INFO]\u001b[0m - tokenizer config file saved in ./tokenizer/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-01-11 17:55:18,608] [    INFO]\u001b[0m - Special tokens file saved in ./tokenizer/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"模型保存\n",
    "\n",
    "在模型训练完成后，需要将模型和优化器参数保存到磁盘，用于模型推理或继续训练。\n",
    "另外，可以将tokenizer保存下来以备后用。\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"ernie_for_sequence_classification\"\n",
    "\n",
    "paddle.save(model.state_dict(), \"{}.pdparams\".format(model_name))\n",
    "paddle.save(optimizer.state_dict(), \"{}.optparams\".format(model_name))\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73fbd321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['财经']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"模型预测\n",
    "\n",
    "在使用模型预测之前，依然要将原始文本数据做Token转化和批次样本切分的操作\n",
    "然后使用训练好的模型计算新闻在各个类别上的概率分布，选择概率最大的那个ID，转换成类别标签输出\n",
    "\"\"\"\n",
    "\n",
    "def predict(data, id2label, batch_size=1):\n",
    "    examples = []\n",
    "    # 数据处理\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # 将数据按照batch_size进行切分\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    # 使用模型预测数据，并返回结果\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [id2label[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n",
    "\n",
    "data = [{\"text\":\"重磅数据公布！2022年存款增超26万亿，超额储蓄待释放！\"}]\n",
    "\n",
    "id2label = dict([(items[1], items[0]) for items in label2id.items()])\n",
    "results = predict(data, id2label)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
